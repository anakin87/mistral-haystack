 Ciao e benvenuti nella puntata 183 del Pointer Podcast, torniamo oggi con degli ospiti, ma prima vi introduto Eugenio, ciao Eugenio. Ciao Luca. Come va? Tutto bene? Tutto bene, tutto bene. Oggi abbiamo due ospiti che arrivano dalla stessa azienda, che è una azienda che produce una libreria che probabilmente avrete sentito nominare nell'ultimo periodo, perché finalmente oggi abbiamo due persone che ne sanno qualcosa di LLM, un argomento che abbiamo toccato molto nell'ultimo periodo, ma noi siamo un po' così sempre un po' appassionati, invece abbiamo due esperti del settore e uno dei due è un nostro ascoltatore e ci abbiamo messo un po' per convincere, ci abbiamo messo perché io l'avevo detto inizialmente, credo che era fino a agosto, quando per la prima volta io ho proposto l'intervista, poi ci abbiamo messo un po', poi abbiamo detto no, forse è meglio aspettare che esce la beta della versione 2 di quello di cui parleremo poi. E quindi, oh beh, dol benvenuto in ansiotto a Stefano Fiorucci, ciao Stefano. Ciao, grazie. E anche a Sara Zanzotera. Ok, sono entrambi dei NLP engineer in DeepSet che se non lo conoscete è l'azienda che lavora su iStack, credo che hai detto già un po' di loro, ma adesso vi lascio la parola per rintrodurvi un po' e dirci quello di cui vi occupate e poi passiamo un po' a parlare di iStack. E intanto grazie per aver accettato l'invito ovviamente. Grazie a voi, come diceva Luca, sono un fan del Point-er Podcast, mi chiamo Stefano Fiorucci. E dopo una formazione in ambito ingegneristico, ho sempre avuto la passione per l'informatica e quindi mi ero affacciato sulla Data Science, ho scoperto la passione per il Mashi Learning e il Natural Language Processing e ho iniziato a lavorare in questi settori e adesso lavoro per DeepSet e mi occupo appunto del framework iStack. Ok, per quello che mi riguarda io sono Sarazza Zotera e sono stata invitata al podcast da Stefano, quindi io in realtà non conoscevo il podcast ma sono molto felice di essere qui in realtà. Facciamo lo stesso lavoro e il mio percorso è stato un po' più lineare di quello di Stefano e io mi sono lavorata già in ingegneria informatica al Politecnico di Milano ormai cinque anni fa, sapevo già che volevo andare più o meno nella direzione del Mashi Learning, però inizialmente ho fatto un dettaglio iniziale perché ho fatto una internship al CERN che non c'entrava niente col Mashi Learning, però intanto ho iniziato a prendere familiarità con Python, con software developer, etc. e poi una volta finita la internship al CERN, ho poi fatto, mandato il mio curriculum di Ipset e sono stata assunta per lavorare piuttosto a questo framework NLP che è iStack. In questo è un po' il mio percorso. Quindi il tuo percorso è stato un po' più lineare. Stefano invece mi raccontava che lui è arrivato in modo a lavorare su iStack in modo un po' particolare forse, ma un modo che secondo me può essere anche di ispirazione un po' per altre persone che magari non contribuiscono soltanto a questo progetto, ma contribuendo anche ad altri progetti possono riuscire a fare più o meno quello che fa anche tu. Ci dici un po' come sei arrivato a lavorare su iStack e poi a lavorare anche proprio per iIpset? Sì, allora io ero interessato al natural language pros, singe, già mi occupavo di progetti di question answering, ma ero a volte frustrato con approcci antichi o non totalmente funzionanti e quindi al codemotion senti parlare di questa libreria e iStack e iniziai ad esplorarla e poi ho detto, va bene, mi sembra interessante un progetto concepito bene, voglio provare a costruirci qualcosa sopra e quindi avevo in quel periodo mi stavo guardando per la prima volta alla serie TwinPix e quindi mi ero costruito una piccola applicazione di question answering estrattivo sulla wiki di TwinPix usando iStack e mi è venuto in mente anche di presentare questa applicazione al Python e al Python ho scoperto che nella platea c'erano degli sviluppatori di iStack quindi è stata anche un'esperienza un po' imbarazzante perché io parlavo di questa libreria ma loro l'avevano sviluppata però è andato tutto bene, ho iniziato anche a conoscerli ho pensato a, di iStack forse è una realtà interessante e da lì ho iniziato anche a contribuire alla libreria in realtà il mio percorso di contribuzione è stato molto interessante e anche formativo per me perché non sempre lavorando in azienda uno rispetta le best practices dei progetti open source quindi ho imparato a fare molte cose mi sono appassionato sempre di più al progetto e sono rimasto in contatto e quindi niente quando si aperta una posizione mi sono candidato e sicuramente dal punto di vista tecnico il fatto di aver contribuito all open source comunque mi ha aiutato perché avevo già dimostrato una certa competenza e niente è andata semplicemente così però in generale anche se contribuire all open source sicuramente può richiedere molto tempo l'ho trovata veramente un'esperienza che mi ha permesso di approfondire molto alcuni aspetti tecnici a programmazione in python ma anche proprio ambito natural language processing quindi in generale consigliere a chiunque di trovare un progetto interessante o comunque con cui si trovano a lavorare e che mi piace iniziare con qualche piccola issue a contribuire penso che questo sia uno degli spot migliori abbiamo riguardo al contribuirlo open source da quando abbiamo il podcast perché molte volte ne abbiamo parlato di progetto open source ma non so se ma forse mi ricordo male ma forse credo che sia la prima volta che abbiamo qualcuno che è partito contribuendo dall'esterno e poi è stato assunto direttamente dall'azienda che insomma produce quel framework e è una cosa che è secondo me molto bella sì e alla sintesi anche del network alle conferenze mi sembra veramente la sintesi di tutto quello che in tanti ci hanno detto che è super importante e mi sembra che è vero a maggior ragione visto quello che c'era contato tra l'altro mi ricordo quel paico quando avevi presentato quel credo che c'eravamo incontrati proprio a quello dell'anno al paio non presentavi qualcosa su stream lit forse si si si è venuto vederti ed è interessante anche dal punto di vista dell'azienda perché noi noi di dipson non ci saremo mai accorti che stefano stava lavorando con a istax se non avessimo se non fossimo andati al paicon quello era il primo paico in italia ma penso in generale è cui di psett partecipava non avevamo sponsorizzato perché l'azienda era veramente piccola ancora quel punto era tipo 20 persone per cui era proprio il primo paico a cui siamo andati e se non ci fossi mandati ci saremo persi questa cosa magari non avrebbe funzionato per cui anche le anche le start up dovrebbero veramente essere proative con tutte queste conferenze quelle cose di andare a cercare questo genere di talento perché è il modo migliore per trovarlo vero vero e prima di capire meglio a cosa serve a istax cosa fa istax vi chiederei un attimo di parlare di lm cosa sono come sono abdestrati perché noi ogni tanto in puntata ne parliamo però sono sicuro che voi anche in poco tempo lo spiegate meglio di come abbiamo fatto ne in questo tempo non so se sarah vuoi rispondere tu o sefano insomma stefano mi sembra l'esperto in questo caso vai no allora in generale le definizioni a parte che io vi ascolto e apprezzo le vostre capacità di vulgative spero di dare un contributo comunque quando parliamo di la large language model oggi che è un termine che ha un po di hype e non è strettamente tecnico scientifico comunque ci riferiamo principalmente dei modelli transformers generativi di grandi dimensioni e solitamente parliamo quindi di modelli costituiti da un solo decoder come quelli della famiglia appunto gpt ma in qualche caso anche da un encoder e un decoder ad esempio quelli della famiglia t5 che erano dei modelli rilasciati da google comunque la caratteristica principale di questi large language model generativi è che dato un testo che solitamente viene chiamato prompt generano in maniera autoregressiva altro testo e è interessante parlare un attimo di come sono addestrati la prima fase che quella che i comuni mortali e aziende non si possono permettere è il pre training in cui il transformer viene addestrato su dei grandi corpora linguistici in maniera non supervisionata ed è questo che costituisci con un scienza di base del transformer e dopo questa fase il modello sa soltanto generare del testo che è statisticamente plausibile sostanzialmente il modello ha preso una distribuzione statistica quindi qui prendo un esempio ti ho trovato in un articolo in questa fase del addestramento dopo il pre training se io scrivo ad un large language model come fare la pizza tra le varie e possibili completamenti potrebbe darmi o una risposta sensata per un essere umano oppure come fare la pizza per una famiglia di sei persone oppure come fare la pizza di che ingredienti ho bisogno quindi dopo il pre training solitamente questo modello non è direttamente utilizzabile quindi c'è una seconda fase che è quello del fine tuning supervisionato e questo sostanzialmente a questo punto il modello viene addestrato su dataset più piccoli e preparati o supervisionati dall'essere umano e viene addestrato a seconda dell'obiettivo che sia seguire istruzioni ad esempio quindi gli viene impartito un compito e deve imparare a seguire istruzioni oppure conversare con l'utente nel caso dei modelli di chat e questa è la seconda fase e poi c'è una terza fase molto importante che è quella dell'allineamento alle preferenze umane e per fare questa per portare avanti questa fase ci sono varie tecniche alcune vengono da reinforcement learning altre sono più semplici però in sostanza si cerca di far sì che il modello allinei il testo che viene generato alle preferenze dell'essere umano e anche qui avviene un ulteriore addestramento quindi solitamente abbiamo queste tre fasi tre fasi pre training supervised fine tuning e allineamento alle preferenze umane e quindi un modello come gpt di open ai o altri modelli open source con capacità simili solitamente sono passati per questi tre fasi e dei modelli così addestrati possono essere già considerati i pronti per l'uso per molte applicazioni possono essere direttamente utilizzati e non necessitano di ulteriore fine tuning ma questo forse lo vedremo meglio poi. Abbiamo visto un po' come vengono addestrati come a cosa serve insomma questi lm noi siamo abituati penso che la maggior parte delle persone abituata a interagirci tramite dei chatbot specialmente diciamo nell'ultimo anno chat gpt è quello che probabilmente viene usato dalla maggior parte delle persone però questi lm ci permettono anche di fare tantissime altre cose ci possiamo come tu fare un fine tuning magari fare qualcosa di molto specifico e renderle molto bravi a rispondere a delle domande sul del testo magari nostro che non abbiamo voglia di pubblicare possiamo creare degli agenti che comunicano tra loro quindi diciamo che abbiamo bisogno di framework che ci aggiungi che ci permettono di fare tutte queste cose e quindi aistak è uno di questi framework ci dite un po' cosa si fa con aistak e perché ci abbiamo bisogno appunto di qualcosa di questo tipo ok allora per cominciare facciamo un riassunto di cosa è aistak in generale aistak si può descrivere come un framework di orchestrazione se vogliamo vederla così quindi permette di costruire applicazioni basate sulle lm uno dei concetti principali che aistak è quello del pipeline in cui il framework offre diversi componenti che poi si possono collegare in modo semplice tra loro in modo a creare poi delle applicazioni più complesse in modo semplice diciamo così alcune di queste applicazioni sono per esempio la ricerca semantica il cosiddetto rag retrieva l'augmenta generation di cui adesso farò un esempio giusto per per capire un po' di cosa si tratta oppure dei chatbot o degli agenti anche quindi il lo scopo di aistak non è tanto quello per esempio di implementare un lm ma di espandere le sue capacità quindi di usare delle lm che per esempio arrivano da gimp face arrivano da open ai e di stenderne le capacità in modo semplice per fare un esempio concreto parliamo di rag appunto rag in inglese si appunto significare triva l'augmenta generation che si può poi tradurre come una generazione aumentata con una ricerca diciamo così le lm soprattutto quelle più piccole tante volte non hanno le conoscenze intrinsche che riguardo agli argomenti che non sono stati inclusi durante il pre training o il time training più tardi quindi per esempio se un argomente molto di nicchia o se un argomente è molto nuovo e non era stato incluso appunto nel data set lm non avrà nessuna conoscenza riguardo un modo per sistemare appunto queste mancanze questa mancanza di conoscenza il fine tuning ma il fine tuning è molto molto costoso è molto difficile da fare richiede hardware specifico richiede un sacco di potenza di calcolo veramente costoso sotto ogni aspetto ci sono in realtà dei modi molto più semplici per come si dice facilitare il lavoro della lm o lo di questi è rag per esempio immaginiamoci che invece immaginiamo di avere una un'applicazione dove invece di mandare direttamente la domanda dell'utente alla lm questa questa domanda viene prima di tutto mandata su google quindi il sistema prende la domanda la manda su google vede quali sono i primi diciamo il primo link si scarica il testo del link e poi lm manda un prompt che assomiglia a questo ti dò un articolo di giornale di una domanda rispondi alla domanda usando l'articolo di giornale in questo modo finché il google è in grado di trovare qualunque informazione relevante riguardo alla domanda dell'utente lm poi non ha da conoscere l'argomento a priori basta basta che si legga l'articolo che io gli ho dato nel prompt e in questo modo è in grado di rispondere alla domanda in modo molto più efficace questa questa tecnica serve serve sia appunto a espandere le capacità dell'lm appunto per raggiungere gli argomenti di cui non è a conoscenza ma è anche molto utile per durare le allucinazioni perché nel momento in cui lm non conosce l'argomento tante volte è prona appunto ad allucinare per cui inventarsi delle risposte che sembrano molto solide magari anche con un ragionamento fittizio dietro ma che in realtà non hanno nessuna base concreta mentre nel momento in cui gli viene data anche un articolo su cui basare la propria risposta lm è molto più come si dice è molto più facilmente è molto più prona a appunto dire guarda l'articolo non non mi risponde a questa domanda oppure a controllare l'articolo prima di formulare la propria risposta per cui non è solo un modo per espandere mai anche per aiutarle a non allucinare questa tecnica è utile perché funziona molto bene anche con lm molto piccole e in se fare una ricerca su internet non è un'operazione come si dice complessa o comunque intensiva dal punto di vista delle risorse della cpu non rilpiene gpu di solito cose di genere per cui è un modo molto leggero per migliorare lm di parecchio e in un certo senso si può anche pensarlo come come vediamola così un esame al libro aperto verso su un esame al libro chiuso cioè un esame al libro chiuso lm deve già sapere di cosa stiamo parlando e non può come si dice se non c'è la risposta si inventa qualcosa come il tipico studente all'esame che si inventa la risposta a caso sperando di azzeccare mentre se è lb aperto è molto più facile perché lm semplicemente va a guardare e è molto più semplice proprio in generale anche a livello cognitivo anche per una persona sarebbe più facile lavorare in questo modo per cui è molto interessante tornando a estac l'idea è che estac facilita la costruzione di un sistema come questo perché in python direttamente non è così ovvio cioè bisogna comunque prima avere delle api che vanno su google a cercarmi un link bisogna scaricarsi il contenuto del link bisogna controllare se il link punta un testo o un video di youtube perché se punta un video di youtube bisogna anche trascrivere il video eccetera eccetera per cui non è semplice estac invece come si dice offre tutti questi componenti già nella libreria c'è un componente per la risciela su internet un componente per per la trascrizione del test un componente per controllare cosa contiene il link eccetera eccetera e tutti questi componenti possono essere poi messi in una pipeline uno dopo l'altro oppure anche con dei branching nel caso se ci siano delle opzioni tra cui scegliere e dovrebbe rendere la costruzione di questi sistemi molto molto più veloce per cui questo è più o meno l'idea del framework e ok sì è tutto quello che volevo dire ok e vai lui no cioè ho giusto una domanda proprio su una delle ultime cose che hai detto riguardo al fatto che ti ci cerca su internet quando cerchi su internet cioè è in qualche modo personalizzabile che tipo di ricerca fai non so apri i primi 10 link vedi soltanto voglio soltanto testo e non audio oppure la seconda parte della donna è posso farlo nel senso fare questa sorta di scraping leggendo questa informazione qualcosa che si può fare oppure ci sono delle limitazioni proprio legali diciamo allora per una cosa la ricerca si può costumizzare per cui sì ci sono per esempio dei parametri per dirmi si ritornami solo i primi 10 link cosa del genere ci sono anche diverse opzioni perché per esempio le ipi di google tante volte ti possono ritornare o dei link delle pagine web oppure già gli snippet di testo quelli che ti ti mostrano nella prima pagina qualche volta per cui ci sono diverse opzioni ma il punto che istac in sé non fa questa decisione per te è più un interfaccia uniforme su delle altre librerie per cui se la libreria ti offre le possibilità di costumizzare la ricerca in modi diversi allora è staccava tutto il possibile per far sì che tu possa usare la libreria oppure e allo stesso tempo possa avere un interfaccia standard per cui collegare questo componente a prescindere la cosa sta facendo da quale liberi ci sia dietro sia comunque semplice da collegare con gli altri perché questa un po è un'astrazione soprattutto un set di librerie che poi si possono usare per aumentare le capacità di un lm no io vorrei aggiungere qualcosa rispetto alla rag cioè l'esempio che ha fatto sarah è anche abbastanza interessante e complesso perché comunque ti rimballo diversi tipologie di dati dal video su youtube al link ovviamente quando uno pensa alla rag pensa anche ad altre casistiche ad esempio usare i tuoi dati come azienda e salvare cioè indicizzare i tuoi dati e salvarteli poi in un database che può essere o un sistema di ricerca testuale come classico come elastic search open search ma anche un database vettoriale non so we did quadrant quindi anche rappresentare queste informazioni in forma vettoriale e ovviamente estac mette a disposizione i componenti per fare anche questo e questa casistica può essere anche interessante anche per le aziende anche grazie alla possibilità di usare dei modelli open source che quindi puoi far girare sulle tue infrastrutture senza inviare i dati a enti esterni eccetera però sicuramente sono anche molte altre applicazioni ma volevo aggiungere questa interessante entrando proprio nel codice della libreria la libreria iskitten python ci sono anche altri linguaggi utilizzati e poi vi chiederei anche avete in mente di rilasciarla anche per altre linguaggi in modo da permettere a più persone di utilizzarla allora e stac in sé è scritto interamente in python perché appunto essendo una proprio un'astrazione sopra altre librerie utilizzare python per interfacciarsi con le librerie è il modo anche più facile per farlo perché tante volte le librerie magari sono implementate per esempio in rastore in altre linguaggi di basso livello più efficienti ma di solito hanno sempre come si dice un'interfaccia in python che poi si può utilizzare facilmente per collegarle con altri quindi e per essa che è molto semplice prendere questa decisione dal punto di vista del linguaggio di essere scritto interamente in python poi chiaramente quando ti tocca installarlo non è così facile perché le librerie poi le altre librerie che vengono utilizzate quelle poi avranno codice in altre in altre linguaggi dal punto di vista del se abbiamo se abbiamo piani per espandere in altre linguaggi al momento no perché in realtà non c'è tutta questa richiesta almeno almeno per altre linguaggi come si dice locali quindi per esempio non abbiamo molte richieste di per implementazioni in go o in rast o in altre linguaggi compilati diciamo così si potrebbe discutere che in effetti c'è c'è un po' di interesse verso avere una versione di aysa magari in javascript on typescript un po' alla linechain per dire ma al momento noi non avremmo la capacità per farlo siamo troppo pochi nel team per cui magari in un futuro quando quando la codebase sarà magari più stabile più matura o magari il team sarà si sarà espansso per includere persone con più capacità verso typescript ma al momento veramente non è nei piani e magari più avanti ok allora visto che la è nominata io soltanto direttamente a quella domanda perché ha nominato l'unchain quindi visto che io personal se vi do dire la verità stefano già l'avevo detto avevo fatto un po' di proe qualche avevo giurato un po' qualche con credo che fosse con gpt for a fatto qualche prova e avevo fatto l'unchain per in quel caso fare una sorta di cioè avevo delle pi tipo e utilizzavo l'unchain per scrivere un po' una risposta in lingua giunosbrale diciamo e quindi avevo visto un po' che c'erano due possibilità c'era l'unchain avevo visto a istaccata fino ad aver usato l'unchain perché mi sembrava un po' più user friendly diciamolo anche la documentazione almeno quella era l'impressione che c'è avuto qualche mese fa quindi vi chiederai quali sono differenze principali che ci sono tra a istacc e l'unchain attualmente allora provo a dire qualcosa io ovviamente ovviamente la mia opinione potrebbe essere parziale quindi cerco di attenermi a dei principi che ci guidano nello sviluppo più che altro sicuramente come scopo e funzionalità sono dei framework simili tra l'altro è uscito da circa tre settimane un mese un paper veramente interessante di fondo una survey su tutti i framework per i large language models perché bene sono comunque un'infinità ognuno anche con un focus diverso e che mette a istacc e l'unchain sulla stessa categoria dei framework lm di base con cui puoi fare sostanzialmente la maggior parte delle operazioni che coinvolgono i large language model e come dicevi tu anche luca l'unchain sicuramente ha catturato grandissima attenzione da parte della community c'è stato molto movimento entusiasmo critiche quello che cerchiamo di fare soprattutto con la release 2.0 di estac che al momento in beta è di attenerci ad alcuni principi quindi rispettare o meglio ispirarsi alla filosofia unix quindi avere dei componenti che fanno una sola cosa e la fanno bene e rendere i componenti aggiuntivi e l'estensibilità della libreria facili da scrivere e poi rendere questi componenti facilmente combinabili in delle pipeline su questo magari ci ritorneremo però una chain può essere generalmente considerata come una pipeline lineare in alcuni casi come ad esempio quello della web retrieva l'augmented generation di cui parlava Sara prima avere una pipeline lineare non è sufficiente e a questo punto se non è sufficiente o crei un'altra strazione oppure se ci sono delle pipeline più flessibili è possibile ridurre il numero della strazione quindi rendere le cose un po più semplici e un altro focus che abbiamo su estac 2 specialmente è un'attenzione alla modularità quindi le integrazione esterne con llm provider database vettoriali sono organizzate in dei pacchetti aggiuntivi quindi uno si stalla solo quello che gli serve e stiamo cercando di ispirarci stiamo cercando di avere attenzione alle developer experience e anche a dare una certa stabilità quindi minimizzare da una da una release all'altra le breaking changes in ogni caso offrire un percorso per fare upgrade quindi ecco questo è quello che ci spira e quindi penso che anche da da questo da questa ispirazione da queste linee guida possa nascere un po una differenza rispetto all'ancene però non non esprimo particolari posizioni personale sicuramente un progetto che stimo con delle caratteristiche simili delle differenze non so sarà se vuoi aggiungere qualcosa di qua no volevo dire molto comprensivo rada dire non avere non avere nulla da aggiungere volevo farmi la domanda poi passiamo a domanda diciamo più corposa sulla versione beta che avete da poco rilasciato così ci dite un po quello che colpino noi da principali però prima volevo chiedervi una cosa perché nominò il fatto anche di usare diverse diversi lm no quindi diciamo che il frame perché agnostico rispetto all lm che vuoi utilizzare visto che comunque il settore mi sembra molto in continua innovazione nel senso che da un giorno l'altro ne escono nuovi praticamente ogni giorno vedo notizia di è uscito un nuovo lm sai questo è forse un po un pelino migliore di quello che era uscito ieri la la domanda è l'utente che vuole utilizzare magari l'ultimissimo che è uscito ieri sera deve aspettare comunque poi introduciate qualcosa e introduciate un support a queste lm oppure può in qualche modo scriversi il supporto in quel caso sarebbe diciamo tipo dovrebbe non so aprire una pure quest e inviare inviare diciamo la proposta o è un qualcosa che semplicemente fai per te e lo puoi usare senza problemi cioè come funziona diciamo quante quanta è espandibile e il stack e anche da un punto di vista di voglio utilizzare lm sul mio sulla mia macchina in quel caso funziona diversamente rispetto a voglio utilizzare un lm che con cui interagisco magari con delle pi questa immagine sia diverso non so quanto sia diverso allora inizio a dire qualcosa poi vediamo sicuramente estendere cioè il supporto a nuovi modelli cioè se vuoi usare il modello che uscito ieri oggi dipende se un modello ad esempio che è stato rilasciato su aggin face quindi è supportato dalla libreria transformers ce lo abbiamo subito perché si interaggiamo direttamente con transformers se è un nuovo modello magari di qualche modell provider se rispetta la se ad esempio noi supportiamo già quel modell provider che ne so o pena io o coir e il modello rispetta la vecchia e pi possiamo integrarlo da subito se hanno cambiato le pi c'è comunque un tempo generalmente proprio perché abbiamo cercato di rendere questi componenti quindi anche i generator che di fondo sono l'astrazione che abbiamo in estac 2 per il large language model abbiamo cercato di tenerla abbastanza semplice quindi dovrebbe essere abbastanza facile anche integrare nuovi large language models per esempio ieri stavo sperimentando con un progetto che ancora non integriamo che è chiamato si chiama oiyama non è proprio un fornitore di modelli ma è una libreria che permette di lanciare il locale alcuni modelli come come se fosse cioè loro introducono questa astrazione chiamata model file che sostanzialmente è un docker file per i large language model e espongono il rest pi e a quel punto per implementare un generator che supporta una casistica del genere cioè io sono messo veramente pochi minuti perché si tratta di fare una chiamata un epi quindi anche nel caso di sviluppo di nuovi del supporta nuovi large language model lo vedo abbastanza semplice e no su questo hai qualcosa di dire sì magari giusto una nota perché effettivamente ha spiegato bene l'argomento una cosa che volevo aggiungere magari è che questo non vale solo per le lele mi regaltava le per qualunque libreria cioè se per esempio tu scopri non lo so vuoi fare invece di fare una ricerca su internet di fare una ricerca su notion o da qualche altra parte e ha bisogno di una libreria specifica che non è ancora integrata con eistac per esempio in teoria dovrebbe essere molto semplice crearsi un proprio componente noi abbiamo il nostro e il nostro develop relationships team che ogni tanto su discord si mette e fa dei live streaming qui creano nuovi componenti per cui in un'ora si creano un nuovo componente per una nuova libreria e poi lo pubblicano per cui tante volte non ci vuole veramente niente a creare una connessione per una piccola libreria come questa che dovrebbe essere anche una delle parti come si c'è importanti del framework di essere così semplice da usare molto carina questa cosa di poter interagire anche con diverse piattaforme non avevo pensato che hai cerco su google ma effettivamente come l'esempio di notion può essere molto molto interessante si e a questo punto io vi chiederei della versione 2.0 beta quali sono le novità principali della del nova release ok allora iniziamo dicendo che novità principali sono tutto quello che riguarda la relizia molto zero magari facciamo un attimo un passo indietro perché eistac non è un framework particolarmente particolarmente vecchio e lo sviluppo è iniziato circa nel 2018 2019 per cui non è un framework molto antico però prima del del avvento dell'LLM di tutto questo questo rumore riguardo al IGPT eccetera il campo dell'NRP era un campo piuttosto di nicchia un po' di ricerca quindi così c'è gli sviluppatori che erano che erano coinvolte nel framework nel campo erano quasi dei ricercatori era un campo abbastanza messice tecnico eistac anche la versione 1.1 e poi tutte le versioni 1.0 qualcosa avevano un po' questo orientamento accademico per cui l'idea era proprio orientato per l'NLP si faceva fine tuning dei modelli BERT eccetera eccetera si faceva extractive QA che è una versione completamente diversa dal generativo AI che abbiamo adesso in cui praticamente il modello andava a estrarre una frase da un articolo per esempio che era la frase che era più più simile alla risposta che un essere umano avrebbe dato alla domanda dell'utente per cui c'era proprio aveva un orientamento completamente diverso che non teneva i modelli generativi molto non dava molto importanza questi modelli generativi possiamo dire per cui appunto poi l'anno scorso quando c'è stato tutto questo questo rumore riguarda c'ha gpt una cosa che abbiamo deciso come team è stato di effettuare una specie di piccolo pivot quindi di muoverci più nel campo del generativo AI invece che restare su questo campo un po' più di NLP estrativo che non era più rilevante anche anche come si dice nell'ambito della ricerca semantica che era poi quello che Insta vuoleva fare come framework per cui quando abbiamo cercato appunto di approcciare il problema abbiamo avuto subito supporto per per l lm anche nel framework 1.x anche nelle versioni 1 per cui non è che non è che era impossibile da fare però in effetti appunto cercando di aggiungere un supporto per lm che fosse semplice da usare e semplice per noi da mantenere si rivelava sempre più complicato anche perché poi lm avevano dei bisogni completamente diversi dai modelli che usavamo in precedenza per cui pochi mesi dopo ci siamo resi conto che probabilmente una riscrittura sarebbe stata necessaria anche anche per aiutare noi stessi a mantenere il framework che vado un po' più efficiente per cui abbiamo iniziato a lavorare marzo di quest'anno alla versione 2.0 che è una riscrittura completa del framework cioè la versione 2.0 parte da nulla parte da zero appunto e poi qui qualche qualche giorno fa abbiamo poi rilasciato la versione beta che è appunto che rimpiazza completamente e il stack 1.0 dal livello di code. A livello di API abbiamo cercato appunto di ereditare tutti i concetti possibili per esempio la versione 1 aveva già il concetto di pipeline aveva già il concetto di avere dei componenti che servevano a connettere librerie diverse a dei modelli nlp in questo caso invece di avere molti modelli nlp ora componente principale appunto è una generativa un generativo language model e d'altra parte però questi concetti sono stati rivisti sono stati potenziati per esempio la pipeline originale era molto più simile a una chain di line chain nel senso che era principalmente lineare o comunque con pochi branching non proprio facile da usare in questo senso mentre la pipeline 2.0 è molto più flessibile molto più facile da usare in caso di branching si possono fare dei cicli eccetera per cui è molto più flessibile. Assumiglia un po di più a una pipeline etl se vogliamo vederla così quindi una cosa molto più molto più flessibile anche la parte dei componenti è diventata molto più semplice perché inizialmente la avevamo pensata proprio dal punto di vista appunto della ricerca nlp per cui avevano così c'è le evi per crearsi un componente erano molto più complicate anche perché ci si aspettava di dover coprire uno use che è diverso mentre ora coltando che si tratta di molte molte librerie piccole molto più semplici anche le API di aistax sono state semplificate drasticamente il che rende tutto molto più semplice sia per noi che per gli utenti in più abbiamo decisamente semplificato il modo in cui gestiamo le dipendenze perché in aistac una appunto la versione 1 di aistac le dipendenze sono sempre state un problema e tendevano a avere molti conflitti perché tendevamo a scaricarle tutte cioè tutti i componenti che avresti mai potuto utilizzare con aistac tendevamo a cercare di scaricarne il più possibile il che non è mai stato una buona idea ma poi con l'espansione del framework è diventato veramente ingestibile e anche tentativi di così ci correggere un po in corso d'opera non hanno funzionato per cui con la versione 2.0 cambiamo completamente approccio quando uno installa aistac gli viene fondito al minimo il minimo possibile di dipendenze poi a seconda di quelle librerie di quali componenti vuole utilizzare si scarica le librerie associate e il framework ti spiega chiaramente quando cerchi di usare un nuovo componente quelle librerie bisogno così può installarle dovrebbe essere più semplice chiaramente speriamo che tutto questo ci aiuti a come si dice che infolge nella community di più quindi implementare un nuovo componente non è più una roba che ti richiede un piecine in linguistica ma è una roba che puoi fare tu in un'ora su uno stream su discord questo almeno sarebbe sarebbe l'idea magari stefano vuoi aggiungere un due parole riguardo alla advento faistac sì appunto per per rintrodurre la versione 2.0 beta per avvicinare le persone per insomma dare una piccola risorsa per l'apprendimento anche insegnare come passare da la 1 alla 2 abbiamo pensato di organizzare un advento faistac per cui adesso nel mese di dicembre abbiamo ogni due giorni pubblichiamo una nuova sfida piuttosto fattibile semplice non come quelle di advento fcod e quindi pubblichiamo queste sfide che potete trovare anche nel sito di faistac e dopo qualche giorno pubblicheremo la soluzione e quindi può essere divertente per imparare ad usare faistac è stato anche divertente per noi per testarlo e anche per creare le sfide che coinvolgono gli edzi è stato ma immagino allora avete nominato vari volte questo discord che mi sembra di capire che è un po il post in cui diciamo si trova la community di faistac la vevo che delle due cose riguardo la prima è immagino che quando una persona vuole scrivere un componente custom per interagire con notion e poi appunto può in qualche modo condividerlo lo condivide direttamente su discord c'è una sorta di store di componenti di terze parti da altre persone in modo che se voglio fare la stessa cosa non me la devo riscrivere ma la trovo direttamente lì e la seconda parte della domanda è quali sono i componenti o le applicazioni più strane e magari anche più interessanti che avete visto nel tempo in questo server discord o comunque dalla community magari rispondo io alla prima parte se fanno la seconda allora nel momento in cui tu crei un tuo componente appunto puoi poi condividerlo con il resto della community allora per prima cosa chiediamo che di solito e almeno per le componenti molto semplici il codice necessario è piuttosto poco per cui un modo uno dei modi più semplici potrebbe addirittura essere condividerlo come un gist però il modo giusto per condividere i componenti in realtà o almeno quello che di solito usiamo noi quando ci sono dei componenti più interessanti da condividere è di creare un pacchetto un pacchetto python praticamente un pacchetto da caricare su pi pi noi le chiamiamo integrations abbiamo una una collezione di integrations che abbiamo proprio sul nostro sito per cui nel momento in youtube pubblico un componente e puoi poi condividerlo con la community magari perché la liberaria che stai sviluppando tu vuoi magari al pubblicizzare un po la tua integrazione con aistak noi mettiamo tutte queste integrazione sulla nostra pagina dove poi puoi trovare documentazione l'inca al pacchetto e magari qualche esempio eccetera per cui noi già lo facciamo con con i vector stores che noi di solito li mettiamo tutti sul sulle integrations page in modo che poi si possa vedere anche in modo semplice quali vector source sono supportati da aistak e poi tutte queste altre integrazione vanno vanno sempre in questa specie di showcase in cui in cui si in cui sono tutte ben visibili e organizzate in un certo senso però questo non è necessario per dire quindi se tu volessi condividerlo in qualunque altro modo fattibile o appunto qualunque cosa possa venirti in mente non ci sono limiti in questo senso l'importante è che il codice sia presente quando cerchi di utilizzarlo con aistak adesso stavo proprio andando a guardare le integrations che abbiamo e sì la cosa anche interessante a base delle integrations che appunto ce ne sono alcune sviluppate da noi e mantenute da noi e altre sviluppate dalla community per esempio anche l'integrazione di cui parlava Sara prima con notion in forza dalla community ci sono integrazione con altri progetti quindi sì è semplice aggiungere riguarda ai progetti sono stati vari nel tempo sicuramente molti di quelli più molti di quelli più notevoli no più diffusi riguardano soprattutto ricerca di informazioni question anzoring riguardo a quelli più curiosi c'è questo che magari poi posso anche condividere nelle note del episodio si poi mette il link sì che esiste credo anche come a game face space ed è questo fatto con aistak uno una what will mother say è un agente che dato un argomento è un nome utente prima era su twitter ora è stato mi grazie su mastodon scrive un tweet con lo stile dell'utente su quell'argomento quindi sicuramente è non è il progetto più utile però è molto carino perché appunto se questa gente che appunto si interfaccia con l'edi di mastodon tira fuori tweet tira fuori l'argomento poi passa queste informazioni al large language model che cerca di generare un tweet con questo stile parlando poi parlando di progetti più seri forse anche interessanti sappiamo che aistak è usato come tool interno anche da aziende come netflix envidia e quindi insomma sì anche se non sempre abbiamo informazioni dettagliate e poi sicuramente vale la pena citare che comunque dipset come anzienda sviluppa una piattaforma per il large language model per team enterprise a pagamento e questa è una parte di che strazione basata proprio su aistak e questo prodotto ad esempio usato per la ricerca e il questionanza ringenerativo da alcune testate giornalistiche gruppi ditoriali quindi è abbastanza variegato il panorama sia delle cose interessanti divertenti sia di quelle usate comunque per me che sono un po' di fuori di questo mondo quando nominavamo un po' di questi progetti mi sembravano tutte cose che comunque sono molto utilizzabili anche da utenti finali anche magari al di fuori cioè banalmente quella di cui parlava all'inizio del diciamo leggeri i tempi di un certo utente e creare nuovi tweet anche per persone insomma che lo fanno non di lavoro ma che devono avere una presenza social può essere un aiuto non da poco insomma mi sembrano tanti us case anche molto pratici interessanti e bello. A proposito di quest'ultimo punto mi venutano un'altra idea questa fatta da una delle nostre develop per advocate sembra si chiamasse il progetto captionate lì invece l'idea era passare un'immagine a questa applicazione da quale generava un testo da mettere su instagram associato a questa immagine e anche le venivano combinati un modello di generazione di testo a partire dall'immagine con un generato più. Ci sono sicuramente tante possibilità che magari uno non ci pensa però appunto vedere tant' magari anche nel discorde che lo mettiamo qua nelle note della puntata si prendono si può prendere ispirazione per magari un progettino qualcosa che magari ci sembra inutile poi invece la possiamo introdurre o nella nostra pipeline lavorativa o la cosa per esempio dei tweet che diceva il Gennio magari è utile anche per un content creator che invece che limitarsi usare solo chat gpt magari può usare comunque qualcosa di generativo ma che tiene conto anche del mio stile di quello che solitamente scrivo farlo in modo che sia coerente insomma con tutto il profilo non soltanto generare testo così inutile. Prima di passare alla prossima domanda una curiosità per chi non è diciamo molto al dentro del mondo degli lm intelligenze artificiale quante è complicato prendere i stack e cercare di costruire qualcosa sopra? Allora l'idea sarebbe che se tu sai scrivere del python dovrebbe essere possibile c'è proprio leggere primo tutorial e non avere neanche troppe sorprese in teoria questa è la risposta ufficiale in teoria dovrebbe essere così semplice poi chiaramente col fatto che noi adesso abbiamo la beta rilasciata se tu lo facessi proprio adesso finiamo il podcast e vai ad aprire i tutorial potresti incontrare qualche bug perché siamo in beta ma appunto da gennaio in poi quando la relisa è stabile in teoria non dovrebbe più succedere però sì l'idea è questa cioè di non richiedere nulla nessuna conoscenza lm allutente proprio ma lasciarli lasciarli liberi di piuttosto di pensare all'architettura completa cosa l'applicazione fa e poi noi dovremmo prenderci come se c'era responsabilità di farla funzionare una volta che tu ce n'è come collegare i componenti ok e invece ne parlavamo prima che diciamo in questo mondo di novità ne escono una dopo l'altra giorno dopo giorno sempre di più è difficile stargli dietro come vivete questa cosa in che modo riuscita a rimanere al passo con i nuovi modelli che oscano ogni giorno provo a rispondere a iniziare a rispondere io sicuramente non è facile penso che chi in questi tempi lavora in questo settore a volte può anche sperimentare una certa fatica o sì non lo so io a volte me lascio un poco sono molto appassionato quindi me lascio a volte coinvolgere da tutte le notizie che vedo le cose da provare quindi no il problema a volte forse per me più fare selezione perché ma beh principale mezzo che uso in realtà è LinkedIn in cui mi sono curato credo un feed che è reputa abbastanza attendibile quindi trovo tante cose interessanti e un altro mezzo in realtà che uso oltre a lo scambio che può avvenire a volte con initiative interna l'azienda anche la community discord di aistak è fatta comunque di vari appassionati quindi c'è una sezione su paper e modelli in cui le persone liberamente scrivono qualcosa e quindi uso questi soprattutto questi sistemi ma in realtà LinkedIn lo uso anche in senso non saprei in senso opposto cioè se voglio studiare qualcosa a volte prendo l'occasione e dico va beh ci voglio scrivere un post sopra e quindi sono costretto a studiarmelo bene a profondire qualche paper come si distingue cioè come dall'esterno diciamo magari senza approfondire troppo l'argomento cioè se esce un nuovo modello un nuovo modello come distingui senza provarlo senza vedere troppo nel dettaglio se è un qualcosa che effettivamente vale la pena magari per decimbo di tempo o se è una cosa che diceva beh questo è l'ennesimo sempre guarda gli altri però non mi dà niente di più rispetto a quello che già uso solitamente è facile o è qualcosa di non troppo complicato questa è una domanda veramente grossa non lo so poi in realtà cioè essendoci the life in questo settore è comunque difficile perché magari si sono sono usciti anche dei modelli recentemente che io ho reputato interessante e si ho fatto delle prove ma magari non avevano ricevuto grande attenzione pubblica sicuramente e poi la valutazione dei large language model sta affrontando un problema dei crisi comunque perché i benchmark accademici in molti casi sono stati integrati involontariamente nei modelli quindi è difficile io provo a guardare un po' su la leaderboard di aggin face valutare però non non c'è una risposta io non sono in grado di formulare risposta molto sensata si in realtà per quel che mi riguarda bene o male l'approccio più o meno lo stesso di stefano in realtà una de mia progette seguire stefano sull'incredito lui in questo modo lui fa un po' del lavoro per me poi io ho meno roba da leggere io lo vedo sempre attivissimo che scrive posti interessanti sull'ultima novità avevo visto parlava di di quelle le le me francese mi mi stralle si pire parlava di quello quando ancora non credo che ne parlasse nessuno penso no so qualche mese fa già era in estate già ne parlava adesso ne parlano tutti lui ne parlava in estate quindi secondo me se non lo si è vite stefano una buona fonti di informazione scusa se ti ho interrotto mi sembrava giusto dirlo no no assolutamente questo bisogna dirlo di più no ma infatti per me per esempio io ho un po' la tendenza opposta stefano perché per me quando appunto c'è troppa informazione tra cui filtrare tante volte sono un po' come si dice soverchiata da queste cose per cui tendo a avere difficoltà anche a filtrare tutto questo hai tutte queste informazioni per cui appunto trovare qualcuno che pubblica regularmente delle posti interessanti appunto per dire stefano ma anche altre persone avverne poche ma di qualità per me è molto importante perché seguire il campo come si dice proprio in tutti suoi aspetti può essere un lavoro a tempo pieno in facti sono sorpresa di come faccia stefano a stargli dietro però sì cioè non è non è un'impressione cioè non è un'impressione dei singoli persone una roba veramente è un'esplosione di modelli novità cose quasi impossibile stargi dietro vi faccio un altro domanda sempre su questo questo argomento no perché da come da quello che dice a stefano nel vostro lavoro sembra comunque debbiamo di rimanere un po giornati anche sulla parte più tecnica quindi magari leggendo dei paper scientifici non so se si può risponde facilmente sia quantificabile diciamo nel vostro lavoro quanto è di lavoro proprio sul sul sul framework sulle vari librerie che realizzate e quanto è più di ricerca per rimanere al passo di di quello che effettivamente poi va implementato perché immagino che è fondamentale rimanere al passo e se domani asciano una architettura che alternativa magari a quelle che vengono utilizzate attualmente è importante che insomma volete oppiare che magari nel breve tempo riusciate poi anche a introdurla quindi quanto più o meno quanto è che quanto fate diciamo ricerca o cercate di stare al passo con la ricerca e quanto effettivamente lavorate sul codice del di istac allora quello che posso dire è che appunto il campo il campo appunto dell'NLP è cambiato molto e sono cambiate anche appunto la in deep set è proprio cambiata un po anche la dinamica tra quanto quanto facciamo ricerca su cosa facciamo ricerca e come la come alterniamo le due attività perché appunto prima dell'avvento del generative AI il primo che era molto più sulla ricerca per cui facevamo appunto pubblicava un articoli riguardo a questi modelli per cui avevamo proprio della ricerca specifica nel campo adesso col generative AI la competizione è molto più cioè molto più più ampia per cui più il tosto che fare ricerca appunto pubblicare molti articoli comunque sviluppare modelli cosa del genere tendiamo a stare più sul un po più in superficie ovvero di sfruttare le libere che sappiamo sappiamo impreventeranno questi modelli a breve per esempio anche in fezio in questo caso è una di quelle lebrierie su cui fondiamo la gran parte del nostro supporto per l lm noi sappiamo che appunto ha in fez normalmente intervuce supporto molto in fretta per cui tendiamo a piuttosto sfruttare sfruttare appunto il loro lavoro in questo campo questo non significa che non ci sia da fare ricerca perché tante volte bisogna appunto capire quali sono gli ambiti promettenti su cosa investire risorse per esempio c'è stato un periodo in cui tutti parlavano di questi agenti ora non se le sente più molto parlare e in effetti in istac non c'è stato non c'è stato anche molta richiesta per queste per questo genere di architettura per quella fine abbiamo siamo rimasti un po inetro su implementazione e forse stato un bene perché appunto la community non aveva bisogno i clienti di di xerca non avevano bisogno dopo un po l'ip si espenta per cui abbiamo un po tutto questo genere di input siamo ricerca che facciamo noi richieste della community o anche in generale è appunto cercare di tenere un occhio sul sul sul sul panorama del generativo ai e cercare di capire un po in che direzione andare è complicato e in realtà appunto rende alla ricerca che facciamo in dipset un po più superficiale però appunto vista la la complessità del campo e l'espansione che sta avendo è praticamente inevitabile ok ok siamo arrivati più o meno sul finale della della intervista ma prima di chiudere voglio fare una domanda che quando l'abbiamo inserita la scaletta aveva scritto che non era proprio non è una domanda proprio semplicissima qui di esponda perché vi chiediamo di fare qualche qualche previsione diciamo abbiamo detto che è un settore che evolve molto facilmente quindi magari la previsione che facciamo oggi tra una settimana diventa realtà oppure tra una settimana viene addirittura superata quindi vi chiederei dal vostro punto di vista cosa pensate che quali pensate che possono essere nei prossimi mesi perché è inutile parlare di anni in questo caso nei prossimi mesi quali possono essere le novità che potrebbero cambiare in polica art in tavola dal punto di vista del d d diciamo nel settore nel pin il settore specificamente negli lm ci sono alcune tecniche che per esempio è utilizzata per permettere a questi questi strumenti di girare su su anche su un macchine con con risorse limitate e che questa secondo è almeno dal mio punto di vista è una delle cose forse più interessanti però magari il boss punto di vista sicuramente più informato del mio vedo un po che ci stanno delle librerie che ti permettono di usare qualche qualche lm su ho visto la operati anche sui sui nhm 1 ci stanno spesso dei brili che ti permettono di fare queste cose vedo un po che c'è un si va verso in questa direzione anche per altri tipi di modelli lavorano su altri tipi di dati whisper ma anche quelli di generazione di immagini si tende un po a fare in modo che possano girare anche su su su su un macchine insomma normali no anche usiamo nella vita tutti i giorni perché altrimenti diventa rimane un qualcosa che è bello ma se ci ha bisogno di un server con 12 g più e farò girare forse rimane un po troppo un po poco utilizzabile dal dal nella vita reale quindi insomma quali sono dal boss un tista le cose le quelle che vi aspettate quello che sperate quello che pensate insomma che possa cambiare un po le carte in tavolo nei prossimi si allora in realtà approfitto di questa domanda per dire alcune delle cose che ho trovato più interessanti ultimamente che potrebbero non essere quelle dei prossimi mesi però tra cui questa che hai detto luca deve far girare i large language model su macchine con risorse limitate cioè è stato qualcosa che veramente ma impressionato perché comunque un anno fa un modello da 7 miliardi di parametri se lo caricavi in alt precision si volevano comunque 14 16 giga di virtual ram della gpu ad oggi riesci con la versione quantizzata a 4 bit riesci comunque a farlo stare con una vertita non così grande di qualità in 4 giga di virtual ram della gpu e la cosa che mi ha sorpreso è stato il fatto che ci il settore ci è arrivato un po grazie sicuramente a ricerca universitaria un po perché ci sono stati dei appassionati dei practitioners dei linguaggi quindi vicino alla macchina da cpu rast che hanno cercato di ottimizzare questi aspetti quindi anche persone che venivano da altri settori e sta cosa per me è stata anche veramente entusiasmante proprio vedere quello che si è mosso nella community in generali e da questo punto di vista quindi oggi abbiamo vari metodi di quantizzazione che quindi permettono di ridurre quelle che sono le risorse necessarie per far girare questi modelli e quindi abbiamo varie tecniche adatte alla gpu e ce ne abbiamo alcune appunto per la cpu quindi e appunto per la cpu ad esempio c'è questo formato gguf del progetto yama cpp e magari riguardo alla quantizzazione che quindi permette di far girare questi modelli con risorse molto limitate magari posso condividere poi un link da mettere in note del episodio che è un articolo molto adatto ai principianti per introdurre a questo tema ma soprattutto con del codice vuoi usare yama da 7 miliardi di metri sul tuo mecco sul tuo altro computer usa questo codice quindi molto interessante e altra altra tendenza che sicuramente vedo è quella dall'atopposto di avere delle tecniche delle librerie per l'inferenza efficace dei large language model dico dall'atopposto perché sono tecniche e librerie che servono a chi fa girare questi modelli in produzione per scopi per scopi commerciali e quindi da questo punto di vista in particolare ci abbiamo dei librerie che sono bu lm e tgi di aggin face e anche questo veramente merita secondo me di essere approfondito perché quelli di bu lm hanno hanno introdotto questo concetto di page attension che è molto interessante perché ottimizza la gestione di una parte della memoria e transformer prendendo in prestito dei concetti da la teoria dei sistemi operativi quindi proprio un altro ambito e vedo che sicuramente anche per usare nella pratica professionale questi modelli anche questa direzione la vedo molto promettente e come per quello che mi riguarda come ultimo aspetto vorrei citare ovviamente i modelli open source che in parte stanno raggiungendo delle prestazioni paragonabili a quelle dei modelli closed e in particolare secondo me un paper molto interessante da leggere è il tecnica report di aggin face in cui spiegano come hanno addestrato zefir il loro recente modello che sostanzialmente sono riusciti mediante una serie di tecniche a distillare molte delle caratteristiche dei modelli dimensioni molto maggiori in un modello molto piccolo sfruttando in maniera sensata l'allineamento alle preferenze umane però con dei metodi poco basati su supervisione umana quindi di partire da un modello come gpt4 o gpt3.5 turbo e tramite una serie di passaggi comunque automatizzati riuscire a passare a un modello più piccolo che ne replica alcune capacità e che è utile per l'utente finale e hanno anche rilasciato non solo il report ma anche proprio la ricetta disponibile su gtab che qualcuno può replicare con un budget neanche troppo grande qualche legento dollari cose così questo questo è il mio interesse ma non so se sarà il futuro ma ti spero soprattutto il opensource. Sì l'open source mi sembra che sta avendo un impatto importante insomma su questo settore cioè non è solta non è guidato soltanto da big come open ai per esempio o microsoft ma anche meta per esempio oppure se non è una big diciamo nel settore ha lavorato più nel diciamo nella parte open source o rilasciando gli ama e quindi mi sembra che diciamo il trino sia più proveniente dal dal mondo opensource in questo mondo diciamo degli lm al momento almeno non so se sarà vuoi aggiungere qualcosa altro su sul futuro che vedi in questo settore in realtà stefano come al solito è riuscito a coprire l'argomente in modo molto molto esauriente appunto sì volevo magari aggiungere che la distinzione tra i modelli opensource e quelli proprietari è una è come si c'è una tensione che non si risolverà in fretta perché appunto anche il fatto che compagnie come open ai ha inizialmente rilasciava dei modelli ora hanno smesso di farlo significa che questa tensione probabilmente è destina a restare poi come si risolverà è interessante perché in realtà è anche molto impressionante la qualità e la varietà dei modelli opensource che continuano a essere rilasciati nonostante il costo che sia il costo del appunto del training e del fight tuning nonostante appunto la difficoltà del compito comunque la comunità opensource sta tenendo il passo bene o male per cui è una cosa molto interessante su cui non potrei fare previsioni al momento ma che è veramente anche entusiasmante da seguire come al momento si guarda non non invito troppo chi lavorò in questo settore perché mi sembra veramente impossibile stare al passo e c'ho dei colleghi che stanno facendo appunto il dottorato proprio su lm diciamo su sull'arguento lm ora non lo specifico non so bene di cosa si occupano però vedo e ho sentito insomma che leggono continuamente per perché ogni giorno la loro ricerca rischia di essere praticamente mangiata da qualcuno che ha pubblicato praticamente potrebbe pubblicare la stessa cosa e quindi non è un settore che diceva beh sono abbastanza tranquillo che oggi inizia a lavorare su una cosa magari per i prossimi sei mesi so abbastanza tranquillo che la posso pubblicare perché nessuno la farà cioè mi avverizzi a lavorare su una cosa e dopo una settimana diventa già beccia quindi sì non è proprio una cosa che che auguro però è allo stesso tempo è bello perché è un settore in continuazione quindi siete sempre diciamo siete sempre riuscite sempre a lavorare con qualcosa di molto recenti di innovativo e di seguire e seguire questa innovazione penso che sia anche divertente non so se immagino almeno dal mio punto di vista sarebbe divertente vedere un po e avere la possibilità di provare di continuo cose nuove questo sicuramente sì allora allora tutti i link che hanno minato Stefano e che hanno minato Sara li troverete poi nel noto della puntata qualcuno lo sono già salvato prima di concludere non so se vuole di lasciare qualche contatto abbiamo detto del proprio link di di stefano non so tu saras e c'hai qualche social su cui sei piuttiva o sì anche io sono attiva sia sul link di inchissi twitter recentemente di solito punto posto contenuti riguardi a riguardo a eistac anche il fatto di questa nuova release c'è molto contenuto anche anche per spargere un po la voce della nuova release per cui sì mi trovate su entrambe sono attiva anche su mastodon ma di solito lo uso appunto in parallella twitter per cui i due contenuti solido si assomigliano molto e bene o male mi trovate lì tu stevano link in la principale link di di tab come anacchino 87 metteremo anche questo nel noto della puntata allora dai allora io vi ringrazio ancora veramente per la vostra disponibilità siete stati abbiamo vediamo tutta non mi ero riso conto perché è un'ora e un quarto già non mi ero riso conto perché mi stava prendendo abbastanza la discussione quindi non mi ero riso il tempo che era passato quindi scusate mi c'è scusate ci abbiamo preso un po più tempo di quanto magari l'avrei visto quindi grazie davvero per la vostra disponibilità e se doveste venire al paio anche quest'anno penso di c'è provo a mandare una un tolcanchi quindi immagino che potrebbe essere la possibilità di incontrarsi penso potrebbe se ci siano anche voi ci incontreremo di nuovo dal vivo o stela o e con sala se ci siano anche tu insomma e grazie ancora e comunque vi salutiamo e vi ringraziamo e ci sentiamo nella prossima puntata del podcast ciao a tutti ciao ciao a tutti ti ringraziamo per aver ascoltato il pointer podcast e ci auguriamo la puntata sia stata di tuo gradimento sia così ti chiediamo di condividere questa puntata sui social e di far conoscere il nostro progetto ai tuoi amici e colleghi se non vuoi perderti i prossimi episodi iscritti al nostro podcast il pointer podcast è disponibile su apple podcast google podcast e spotify se ci segui su apple podcast ti chiediamo di scrivere una recensione ci aiutere a crescere e a raggiungere un pubblico sempre maggiore se vuoi contattarci puoi farlo tramite email all'indilizzo info che ho a ce la pointer podcast punto it o tramite social siamo su twitter facebook linklin ed instagram grazie per averci ascoltato e alla prossima